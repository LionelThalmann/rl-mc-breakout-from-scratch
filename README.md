# Monteâ€‘Carlo Control Agent for a Breakout Clone

<a href="https://www.python.org"><img src="https://img.shields.io/badge/python-3.9%2B-blue.svg" alt="Python version"></a>

---

## ğŸ® Project Summary

This project implements **onâ€‘policy Monteâ€‘Carlo Control** to learn an optimal policy for a gridâ€‘based Breakout clone developed for the *Reinforcement Learning* exercise at TUÂ Wien. A minimal custom environment (`game_env.py`) models the ball, paddle and brick collisions; an `MC_agent` learns from episodic returns using Îµâ€‘greedy exploration and incremental Qâ€‘value updates.  The training script performs a small hyperâ€‘parameter gridâ€‘search and stores plots that illustrate convergence behaviour and the learned ball trajectories.

**Learning Focus**  
The focus of this exercise was to build the game environment from scratch, code the Monteâ€‘Carlo agent, and design the training pipeline using pure Python and NumPy - rather than relying on external RL frameworks or aiming for productionâ€‘grade performance.


---

## ğŸ“‚ Repo Structure

```text
â”œâ”€â”€ game_env.py          # Minimal, custom Breakout environment (no external deps)
â”œâ”€â”€ MC_agent.py          # Onâ€‘policy Monteâ€‘Carlo agent
â”œâ”€â”€ train_script.py      # Training loop, gridâ€‘search & plotting
â”œâ”€â”€ plots/               # â†– autogenerated; store all PNG results here
â”‚Â Â  â”œâ”€â”€ trajectories_*.png
â”‚Â Â  â”œâ”€â”€ grid_*_.png
â”‚Â Â  â””â”€â”€ ...
â””â”€â”€ README.md            # <â€“â€“ you are here
```
---

## ğŸ— Environment Details

| Parameter     | Value                                                | Notes                                          |
| ------------- | ---------------------------------------------------- | ---------------------------------------------- |
| Grid size     | 14Â Ã—Â 10                                              | Discrete cells; origin at topâ€‘left             |
| Brick layouts | `checker`, `pyramid`, `wall`                         | Each tested with 2 &Â 4 brickâ€‘rows              |
| Paddle size   | 5Â Ã—Â 1                                                | Actions: `â€‘1` (left), `0` (stay), `+1` (right) |
| Ball velocity | *vx*Â âˆˆÂ {â€‘2â€¦2}, *vy*Â âˆˆÂ {â€‘1,1}                         | Horizontal speed adjusted on paddle hit        |
| Reward scheme | â€‘1/step, +1 per brick, +10 on clear, **â€‘10 on miss** | Sparse but shaping encourages long rallies     |

The environment logic - including collision handling and reward computation - is encapsulated in **`game_env.py`**.  The `conf()` helper produces level configurations dynamically so the same class supports multiple layouts.

---

## ğŸ”¢ State Representation

A naÃ¯ve tabular MC approach would explode in stateâ€‘space size.  I therefore discretise the raw observation into a compact, hashable tuple:

1. **Ball position:** bucket columns/rows into 2â€‘cell bins.
2. **Ball velocity:** horizontal speed shifted to nonâ€‘negative range; vertical direction mapped to {0,1}.
3. **Paddle offset:** discretised distance between ball column and paddle centre.
4. **Brick counter:** cap remaining bricks atÂ 15 to limit state cardinality.

```text
state = (
    ball_col_bin,
    ball_row_bin,
    ball_vx_shifted,
    ball_vy_dir,
    paddle_offset_bin,
    bricks_left_capped,
)
```

This encoding empirically balances **granularity** (enough to learn precise paddle timing) and **generality** (manageable Qâ€‘table size).

---

## ğŸ¤– Agent & Learning Algorithm

* **Policy:** Îµâ€‘greedy over current Qâ€‘values
  Îµ is annealed each episode (`Îµ â† max(0.05, ÎµÂ·0.9995)`).
* **Return estimate:** Fullâ€‘episode return `G_t = Î£ Î³^kÂ r_{t+k}` (firstâ€‘visit MC).
  Either *sample average* (Î±Â =None) or *constantâ€‘step* Î±Â âˆˆÂ {0.05,Â 0.10,Â 0.20}.
* **Discount:** Î³Â âˆˆÂ {0.90,Â 0.95,Â 0.99}.

For every `(state, action)` pair the Qâ€‘table is updated incrementally:

```
Q â† Q + Î± Â· (G âˆ’ Q)
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training & Hyperâ€‘parameter Search

```python
# train_script.py (excerpt)
grid = {
    "eps"  : [1.0, 0.5, 0.1],
    "gamma": [0.90, 0.95, 0.99],
    "alpha": [0.05, 0.10, 0.20],
}
```

* **Episodes per run:**Â 10Â 000
* **Total runs:** `3Â Îµ Ã—â€¯3Â Î³ Ã—â€¯3Â Î± Ã—â€¯3Â layouts Ã—â€¯2Â brickâ€‘rowsÂ =Â 162`Â training jobs.
* **Metrics logged per episode:** total return, steps, bricks broken, runtime.

At the end of each run I roll out **greedy trajectories** (ÎµÂ =Â 0) at five initial horizontal velocities `vx0Â âˆˆÂ {â€‘2,â€‘1,0,1,2}` to visualise the learned strategy.

---

## ğŸ“Š Results & Figures

```markdown
### Runtime Performance
![Runtime](plots_f/runtime.png)

### Gridâ€‘search Performance (mean return, lastÂ 100Â episodes)
![Gridâ€‘search Wall](plots/grid_wall_4_2.png)
![Gridâ€‘search Checker](plots/grid_checker_4_2.png)
![Gridâ€‘search PyramidÂ ](plots/grid_pyramid_4_2.png)

### Learning Curves (example: Wall, 16Â bricks)
![Episode returns](plots/rewards_wall_16bricks_non_smoothed.png)
![Cumulative score](plots/scores_wall_16bricks_non_smoothed.png)

### Greedy Ball Trajectories
![Trajectories - Checker](plots/trajectories_checker_4.png)
![Trajectories - Pyramid](plots/trajectories_pyramid_4.png)
![Trajectories - Wall](plots/trajectories_wall_4.png)
```
---

## ğŸš€ Getting Started

```bash
# 1.Â Clone your private repo
$ git clone <yourâ€‘url>/breakoutâ€‘mc.git && cd breakoutâ€‘mc

# 2.Â Create environment & install deps
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 3.Â Train the agent & reproduce plots
$ python train_script.py
```

### `requirements.txt`

<details>
<summary>(click to expand)</summary>

```
matplotlib
numpy
seaborn
tqdm
scipy
```

</details>

*(The environment itself is dependency-free; only plotting & progressâ€‘bars require extra libraries.)*

---

## âœ¨ Key Takeâ€‘aways

* **State abstraction** was critical - bucketing reduced the searchable stateâ€‘action space fromÂ â‰ˆ10â¸ possibilities to â‰ˆ10âµ without harming performance.
* The agent discovered layoutâ€‘specific strategies:
  *horizontal wallâ€‘grinding loops* for **`wall`**, *surgical shots* for **`pyramid`**, and *general defensive play* for sparse **`checker`** levels.
* Monteâ€‘Carlo control, though simple, achieved nearâ€‘optimal play in <â€¯15Â minutes on a laptop CPU for all tested layouts.

